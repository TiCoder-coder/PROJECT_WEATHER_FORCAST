"""
CRAWL_DATA_FROM_VRAIN_BY_SELENIUM.PY
====================================

Script crawl dá»¯ liá»‡u thá»i tiáº¿t tá»« VRAIN.VN báº±ng Selenium (trÃ¬nh duyá»‡t tá»± Ä‘á»™ng)

Má»¥c Ä‘Ã­ch:
    - Láº¥y dá»¯ liá»‡u tá»« cÃ¡c tráº¡m Ä‘o VRAIN qua giao diá»‡n web
    - Sá»­ dá»¥ng Selenium Ä‘á»ƒ Ä‘iá»u khiá»ƒn Chrome headless (tá»± Ä‘á»™ng nháº¥p chuá»™t, scroll, v.v.)
    - PhÃ¹ há»£p khi API khÃ´ng sáºµn hoáº·c cáº¥u trÃºc HTML phá»©c táº¡p

Äáº·c Ä‘iá»ƒm:
    - Sá»­ dá»¥ng Selenium + Chrome headless (khÃ´ng cáº§n giao diá»‡n Ä‘á»“ há»a)
    - Äa luá»“ng (ThreadPoolExecutor) Ä‘á»ƒ crawl 64 tá»‰nh song song
    - Xá»­ lÃ½ timeout, lá»—i káº¿t ná»‘i, retry tá»± Ä‘á»™ng
    - Chuáº©n hÃ³a dá»¯ liá»‡u Tiáº¿ng Viá»‡t (Unicode normalization)
    - Xuáº¥t Excel vá»›i Ä‘á»‹nh dáº¡ng Ä‘áº¹p

CÃ¡ch sá»­ dá»¥ng:
    python Crawl_data_from_Vrain_by_Selenium.py
    
    # Hoáº·c tá»« Django view:
    crawler = VrainCrawlerFinal(headless=True, max_workers=5)
    crawler.run()

Dá»¯ liá»‡u Ä‘Æ°á»£c lÆ°u:
    - Excel: output/Bao_cao_YYYYMMDD_HHMMSS.xlsx
    - Ghi log chi tiáº¿t vÃ o console + file

Biáº¿n cáº¥u hÃ¬nh:
    - headless: True = cháº¡y trong background, False = hiá»ƒn thá»‹ browser
    - max_workers: sá»‘ luá»“ng (5-10 há»£p lÃ½, trÃ¡nh quÃ¡ táº£i server VRAIN)
    - max_retries: sá»‘ láº§n retry náº¿u káº¿t ná»‘i tháº¥t báº¡i

LÆ°u Ã½:
    - Cáº§n cÃ i ChromeDriver phÃ¹ há»£p vá»›i phiÃªn báº£n Chrome hiá»‡n táº¡i
    - Cáº§n thÆ° má»¥c output/ cÃ³ sáºµn
    - Selenium cháº­m hÆ¡n API/requests nhÆ°ng linh hoáº¡t hÆ¡n

Dependencies:
    - selenium: Ä‘iá»u khiá»ƒn trÃ¬nh duyá»‡t
    - pandas: xá»­ lÃ½ dá»¯ liá»‡u
    - openpyxl: xuáº¥t Excel
    - beautifulsoup4: parse HTML
    - threading: Ä‘a luá»“ng
    - unicodedata: xá»­ lÃ½ Tiáº¿ng Viá»‡t

Author: Weather Forecast Team
Version: 1.0
Last Updated: 2026-02-06
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, WebDriverException
import pandas as pd
import time
import json
import os
import re
import unicodedata
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading


class VrainCrawlerFinal:
    def __init__(self, headless=True, max_workers=5, max_retries=3):
        self.base_url = "https://www.vrain.vn"
        self.all_rainfall_data = []
        self.headless = headless
        self.max_workers = max_workers
        self.max_retries = max_retries
        self.data_lock = threading.Lock()
        self.unique_stations = {}
        self.failed_provinces = []

    def create_driver(self):
        chrome_options = Options()
        if self.headless:
            chrome_options.add_argument("--headless=new")

        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_experimental_option(
            "prefs", {"profile.default_content_setting_values": {"images": 2}}
        )
        chrome_options.page_load_strategy = "eager"

        driver = webdriver.Chrome(options=chrome_options)
        driver.set_page_load_timeout(30)
        return driver

    def normalize_string(self, text):
        """Chuáº©n hÃ³a tiáº¿ng Viá»‡t vÃ  loáº¡i bá» khoáº£ng tráº¯ng thá»«a"""
        if not text:
            return ""
        text = unicodedata.normalize("NFC", text)
        return " ".join(text.strip().split())

    def extract_rainfall(self, text):
        """TrÃ­ch xuáº¥t sá»‘ tá»« chuá»—i '12.5 mm'"""
        match = re.search(r"(\d+[\.,]?\d*)", text)
        return match.group(1).replace(",", ".") if match else "0"

    def get_province_name(self, driver):
        """Láº¥y tÃªn tá»‰nh tá»« cÃ¡c selector khÃ¡c nhau"""
        selectors = [
            "span[_ngcontent-ng-c641299110]",
            "span[_ngcontent-serverapp-c641299110]",
            "span[class*='ng-']",
            ".app-title span",
            "h1 span",
            ".province-name",
            "h1",
            "title",
        ]

        for selector in selectors:
            try:
                element = driver.find_element(By.CSS_SELECTOR, selector)
                text = element.text.strip()
                if text and len(text) > 1:
                    text = re.sub(r"(VRAIN|LÆ°á»£ng mÆ°a.*|[-|])", "", text).strip()
                    if text and not any(
                        x in text.lower() for x in ["táº¡i cÃ¡c tráº¡m", "ngÃ y"]
                    ):
                        return self.normalize_string(text)
            except:
                continue

        try:
            title = driver.title
            if title and "VRAIN" in title:
                text = title.replace("VRAIN", "").replace("-", "").strip()
                if text:
                    return self.normalize_string(text)
        except:
            pass

        return None

    def crawl_province(self, province_id, retry_count=0):
        """Crawl má»™t tá»‰nh vá»›i cÆ¡ cháº¿ retry"""
        driver = None
        try:
            driver = self.create_driver()
            url = f"{self.base_url}/{province_id}/overview?public_map=windy"
            driver.get(url)

            wait = WebDriverWait(driver, 15)
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "body")))
            time.sleep(4)

            province_name = self.get_province_name(driver)

            if not province_name:
                province_name = f"ID_{province_id}"

            found_count = 0
            crawl_time = datetime.now().strftime("%d/%m/%Y %H:%M")

            selectors = [
                "div[class*='station-row']",
                "div[class*='station']",
                "tr.station-item",
                ".station-list-item",
                "table tbody tr",
                "table tr",
                ".data-row",
            ]

            elements = []
            for selector in selectors:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                if len(elements) > 2:
                    break

            for el in elements:
                try:
                    text_content = el.text.strip()
                    if not text_content or "LÆ°á»£ng mÆ°a" in text_content:
                        continue

                    lines = [
                        line.strip()
                        for line in text_content.split("\n")
                        if line.strip()
                    ]

                    if len(lines) >= 2:
                        station_name = self.normalize_string(lines[0])
                        rainfall_val = self.extract_rainfall(lines[-1])

                        unique_key = f"{province_name}_{station_name}".lower()

                        with self.data_lock:
                            if unique_key not in self.unique_stations:
                                record = {
                                    "province_id": province_id,
                                    "tinh": province_name,
                                    "tram": station_name,
                                    "luong_mua": float(rainfall_val),
                                    "thoi_gian": crawl_time,
                                }
                                self.unique_stations[unique_key] = record
                                found_count += 1
                except:
                    continue

            if found_count == 0 or province_name.startswith("ID_"):
                if retry_count < self.max_retries:
                    print(
                        f"âš ï¸  ID {province_id}: {province_name} - KhÃ´ng cÃ³ dá»¯ liá»‡u, thá»­ láº¡i láº§n {retry_count + 1}..."
                    )
                    if driver:
                        driver.quit()
                    time.sleep(2)
                    return self.crawl_province(province_id, retry_count + 1)
                else:
                    print(
                        f"âŒ ID {province_id}: Tháº¥t báº¡i sau {self.max_retries} láº§n thá»­"
                    )
                    with self.data_lock:
                        self.failed_provinces.append(province_id)
                    return 0

            print(f"âœ… ID {province_id}: {province_name} - Láº¥y Ä‘Æ°á»£c {found_count} tráº¡m")
            return found_count

        except Exception as e:
            if retry_count < self.max_retries:
                print(
                    f"âš ï¸  ID {province_id} lá»—i: {str(e)[:30]} - Thá»­ láº¡i láº§n {retry_count + 1}..."
                )
                if driver:
                    driver.quit()
                time.sleep(2)
                return self.crawl_province(province_id, retry_count + 1)
            else:
                print(
                    f"âŒ Lá»—i ID {province_id} sau {self.max_retries} láº§n thá»­: {str(e)[:50]}"
                )
                with self.data_lock:
                    self.failed_provinces.append(province_id)
                return 0
        finally:
            if driver:
                driver.quit()

    def run(self, start_id=1, end_id=63):
        print(f"ðŸš€ Báº¯t Ä‘áº§u crawl tá»« ID {start_id} Ä‘áº¿n {end_id}...")
        print(f"ðŸ”„ Sá»‘ láº§n thá»­ láº¡i tá»‘i Ä‘a: {self.max_retries}")

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            executor.map(self.crawl_province, range(start_id, end_id + 1))

        if self.failed_provinces:
            print(f"\nðŸ”„ Äang retry {len(self.failed_provinces)} tá»‰nh tháº¥t báº¡i...")
            retry_failed = []
            for province_id in self.failed_provinces:
                time.sleep(1)
                result = self.crawl_province(province_id, 0)
                if result == 0:
                    retry_failed.append(province_id)

            self.failed_provinces = retry_failed

        self.all_rainfall_data = list(self.unique_stations.values())
        self.all_rainfall_data.sort(key=lambda x: (x["province_id"], x["tram"]))

    def export(self):
        if not self.all_rainfall_data:
            print("âš ï¸ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ xuáº¥t!")
            return

        df = pd.DataFrame(self.all_rainfall_data)
        df = df.drop(columns=["province_id"], errors="ignore")

        # Äá»“ng bá»™ tÃªn cá»™t chuáº©n schema
        df = df.rename(columns={
            "tinh": "province",
            "tram": "station_name",
            "luong_mua": "rain_total",
            "thoi_gian": "timestamp",
        })

        # ThÃªm cÃ¡c cá»™t cÃ²n thiáº¿u
        if "station_id" not in df.columns:
            df["station_id"] = df["station_name"]
        if "district" not in df.columns:
            df["district"] = ""
        if "status" not in df.columns:
            df["status"] = ""
        if "data_time" not in df.columns:
            df["data_time"] = df["timestamp"]

        # Sáº¯p xáº¿p vÃ  chá»n Ä‘Ãºng thá»© tá»± cá»™t
        schema_columns = [
            "station_id",
            "station_name",
            "province",
            "district",
            "rain_total",
            "status",
            "timestamp",
            "data_time"
        ]
        df = df[schema_columns]

        # Dynamic path: tá»± tÃ­nh tá»« vá»‹ trÃ­ project root
        _project_root = Path(__file__).resolve().parents[2]
        output_dir = str(_project_root / "data" / "data_crawl")
        os.makedirs(output_dir, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        excel_path = os.path.join(output_dir, f"Bao_cao_{timestamp}.xlsx")
        df.to_excel(excel_path, index=False)

        print(f"\n{'=' * 60}")
        print(f"ðŸ“Š Tá»”NG Káº¾T:")
        print(f"ðŸ“ Tá»•ng sá»‘ tráº¡m thu tháº­p: {len(df)}")

        if self.failed_provinces:
            print(
                f"âŒ CÃ¡c ID váº«n tháº¥t báº¡i: {', '.join(map(str, self.failed_provinces))}"
            )
        else:
            print(f"âœ… Táº¥t cáº£ tá»‰nh Ä‘á»u láº¥y dá»¯ liá»‡u thÃ nh cÃ´ng!")

        print(f"ðŸ“„ File Ä‘Ã£ lÆ°u táº¡i: {excel_path}")
        print(f"{'=' * 60}")


if __name__ == "__main__":
    crawler = VrainCrawlerFinal(headless=True, max_workers=3, max_retries=3)
    crawler.run(start_id=1, end_id=63)
    crawler.export()