"""
ğŸ“Š Metrics Module - Äá»‹nh nghÄ©a cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ cho ML models
=================================================================

NÆ¡i Ä‘á»‹nh nghÄ©a cÃ¡c metric: MAE, RMSE, MAPE, R2...
DÃ¹ng chung cho má»i model trong há»‡ thá»‘ng dá»± bÃ¡o thá»i tiáº¿t.

Author: Weather Forecast Team
"""

import numpy as np
from typing import Dict, Union, Optional, List
from dataclasses import dataclass


# =============================================================================
# ğŸ“¦ DATA CLASSES
# =============================================================================

@dataclass
class MetricResult:
    """Káº¿t quáº£ cá»§a má»™t metric Ä‘Æ¡n láº»"""
    name: str
    value: float
    description: str
    
    def to_dict(self) -> Dict:
        return {
            "name": self.name,
            "value": round(self.value, 6),
            "description": self.description
        }


@dataclass  
class EvaluationReport:
    """BÃ¡o cÃ¡o tá»•ng há»£p táº¥t cáº£ metrics"""
    metrics: Dict[str, float]
    target_column: str
    n_samples: int
    
    def to_dict(self) -> Dict:
        return {
            "target_column": self.target_column,
            "n_samples": self.n_samples,
            "metrics": {k: round(v, 6) for k, v in self.metrics.items()}
        }


# =============================================================================
# ğŸ“ REGRESSION METRICS - CÃ¡c metric cho bÃ i toÃ¡n há»“i quy
# =============================================================================

def mean_absolute_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    MAE - Mean Absolute Error (Sai sá»‘ tuyá»‡t Ä‘á»‘i trung bÃ¬nh)
    
    CÃ´ng thá»©c: MAE = (1/n) * Î£|y_true - y_pred|
    
    Ã nghÄ©a:
    - ÄÆ¡n vá»‹ giá»‘ng vá»›i target (mm mÆ°a, Ä‘á»™ C, ...)
    - MAE cÃ ng nhá» cÃ ng tá»‘t
    - KhÃ´ng pháº¡t náº·ng outliers nhÆ° MSE/RMSE
    
    Args:
        y_true: GiÃ¡ trá»‹ thá»±c táº¿
        y_pred: GiÃ¡ trá»‹ dá»± Ä‘oÃ¡n
        
    Returns:
        float: GiÃ¡ trá»‹ MAE
    """
    y_true = np.asarray(y_true).flatten()
    y_pred = np.asarray(y_pred).flatten()
    
    if len(y_true) != len(y_pred):
        raise ValueError(f"Äá»™ dÃ i khÃ´ng khá»›p: y_true={len(y_true)}, y_pred={len(y_pred)}")
    
    return float(np.mean(np.abs(y_true - y_pred)))


def mean_squared_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    MSE - Mean Squared Error (Sai sá»‘ bÃ¬nh phÆ°Æ¡ng trung bÃ¬nh)
    
    CÃ´ng thá»©c: MSE = (1/n) * Î£(y_true - y_pred)Â²
    
    Ã nghÄ©a:
    - ÄÆ¡n vá»‹ lÃ  bÃ¬nh phÆ°Æ¡ng cá»§a target
    - Pháº¡t náº·ng cÃ¡c sai sá»‘ lá»›n (outliers)
    - MSE cÃ ng nhá» cÃ ng tá»‘t
    
    Args:
        y_true: GiÃ¡ trá»‹ thá»±c táº¿
        y_pred: GiÃ¡ trá»‹ dá»± Ä‘oÃ¡n
        
    Returns:
        float: GiÃ¡ trá»‹ MSE
    """
    y_true = np.asarray(y_true).flatten()
    y_pred = np.asarray(y_pred).flatten()
    
    if len(y_true) != len(y_pred):
        raise ValueError(f"Äá»™ dÃ i khÃ´ng khá»›p: y_true={len(y_true)}, y_pred={len(y_pred)}")
    
    return float(np.mean((y_true - y_pred) ** 2))


def root_mean_squared_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    RMSE - Root Mean Squared Error (CÄƒn báº­c hai sai sá»‘ bÃ¬nh phÆ°Æ¡ng trung bÃ¬nh)
    
    CÃ´ng thá»©c: RMSE = âˆšMSE = âˆš[(1/n) * Î£(y_true - y_pred)Â²]
    
    Ã nghÄ©a:
    - ÄÆ¡n vá»‹ giá»‘ng vá»›i target (dá»… diá»…n giáº£i)
    - Pháº¡t náº·ng cÃ¡c sai sá»‘ lá»›n hÆ¡n MAE
    - RMSE >= MAE (luÃ´n luÃ´n)
    - RMSE cÃ ng nhá» cÃ ng tá»‘t
    
    Args:
        y_true: GiÃ¡ trá»‹ thá»±c táº¿
        y_pred: GiÃ¡ trá»‹ dá»± Ä‘oÃ¡n
        
    Returns:
        float: GiÃ¡ trá»‹ RMSE
    """
    return float(np.sqrt(mean_squared_error(y_true, y_pred)))


def mean_absolute_percentage_error(
    y_true: np.ndarray, 
    y_pred: np.ndarray,
    epsilon: float = 1e-10
) -> float:
    """
    MAPE - Mean Absolute Percentage Error (Sai sá»‘ pháº§n trÄƒm tuyá»‡t Ä‘á»‘i trung bÃ¬nh)
    
    CÃ´ng thá»©c: MAPE = (100/n) * Î£|((y_true - y_pred) / y_true)|
    
    Ã nghÄ©a:
    - ÄÆ¡n vá»‹: pháº§n trÄƒm (%)
    - Dá»… hiá»ƒu vÃ  so sÃ¡nh giá»¯a cÃ¡c dataset khÃ¡c nhau
    - âš ï¸ KhÃ´ng dÃ¹ng Ä‘Æ°á»£c khi y_true cÃ³ giÃ¡ trá»‹ 0 hoáº·c gáº§n 0
    
    Args:
        y_true: GiÃ¡ trá»‹ thá»±c táº¿
        y_pred: GiÃ¡ trá»‹ dá»± Ä‘oÃ¡n
        epsilon: GiÃ¡ trá»‹ nhá» Ä‘á»ƒ trÃ¡nh chia cho 0
        
    Returns:
        float: GiÃ¡ trá»‹ MAPE (Ä‘Æ¡n vá»‹ %)
    """
    y_true = np.asarray(y_true).flatten()
    y_pred = np.asarray(y_pred).flatten()
    
    if len(y_true) != len(y_pred):
        raise ValueError(f"Äá»™ dÃ i khÃ´ng khá»›p: y_true={len(y_true)}, y_pred={len(y_pred)}")
    
    # TrÃ¡nh chia cho 0 báº±ng cÃ¡ch thÃªm epsilon
    denominator = np.abs(y_true) + epsilon
    
    # Cáº£nh bÃ¡o náº¿u cÃ³ nhiá»u giÃ¡ trá»‹ gáº§n 0
    near_zero_count = np.sum(np.abs(y_true) < 0.01)
    if near_zero_count > len(y_true) * 0.1:  # > 10% giÃ¡ trá»‹ gáº§n 0
        import warnings
        warnings.warn(
            f"âš ï¸ {near_zero_count}/{len(y_true)} giÃ¡ trá»‹ y_true gáº§n 0. "
            "MAPE cÃ³ thá»ƒ khÃ´ng Ä‘Ã¡ng tin cáº­y!"
        )
    
    mape = np.mean(np.abs((y_true - y_pred) / denominator)) * 100
    return float(mape)


def symmetric_mean_absolute_percentage_error(
    y_true: np.ndarray, 
    y_pred: np.ndarray
) -> float:
    """
    sMAPE - Symmetric Mean Absolute Percentage Error
    
    CÃ´ng thá»©c: sMAPE = (100/n) * Î£(2 * |y_true - y_pred| / (|y_true| + |y_pred|))
    
    Ã nghÄ©a:
    - PhiÃªn báº£n cÃ¢n Ä‘á»‘i cá»§a MAPE
    - Xá»­ lÃ½ tá»‘t hÆ¡n khi y_true hoáº·c y_pred gáº§n 0
    - Giá»›i háº¡n trong khoáº£ng [0, 200]%
    
    Args:
        y_true: GiÃ¡ trá»‹ thá»±c táº¿
        y_pred: GiÃ¡ trá»‹ dá»± Ä‘oÃ¡n
        
    Returns:
        float: GiÃ¡ trá»‹ sMAPE (Ä‘Æ¡n vá»‹ %)
    """
    y_true = np.asarray(y_true).flatten()
    y_pred = np.asarray(y_pred).flatten()
    
    if len(y_true) != len(y_pred):
        raise ValueError(f"Äá»™ dÃ i khÃ´ng khá»›p: y_true={len(y_true)}, y_pred={len(y_pred)}")
    
    numerator = np.abs(y_true - y_pred)
    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2
    
    # Xá»­ lÃ½ trÆ°á»ng há»£p cáº£ y_true vÃ  y_pred Ä‘á»u = 0
    mask = denominator != 0
    smape = np.zeros_like(numerator)
    smape[mask] = numerator[mask] / denominator[mask]
    
    return float(np.mean(smape) * 100)


def r2_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    RÂ² - Coefficient of Determination (Há»‡ sá»‘ xÃ¡c Ä‘á»‹nh)
    
    CÃ´ng thá»©c: RÂ² = 1 - (SS_res / SS_tot)
              SS_res = Î£(y_true - y_pred)Â²
              SS_tot = Î£(y_true - mean(y_true))Â²
    
    Ã nghÄ©a:
    - Tá»· lá»‡ phÆ°Æ¡ng sai cá»§a y Ä‘Æ°á»£c giáº£i thÃ­ch bá»Ÿi model
    - RÂ² = 1: Model hoÃ n háº£o
    - RÂ² = 0: Model dá»± Ä‘oÃ¡n báº±ng mean(y)
    - RÂ² < 0: Model tá»‡ hÆ¡n cáº£ baseline mean
    
    Args:
        y_true: GiÃ¡ trá»‹ thá»±c táº¿
        y_pred: GiÃ¡ trá»‹ dá»± Ä‘oÃ¡n
        
    Returns:
        float: GiÃ¡ trá»‹ RÂ² (thÆ°á»ng trong khoáº£ng [0, 1])
    """
    y_true = np.asarray(y_true).flatten()
    y_pred = np.asarray(y_pred).flatten()
    
    if len(y_true) != len(y_pred):
        raise ValueError(f"Äá»™ dÃ i khÃ´ng khá»›p: y_true={len(y_true)}, y_pred={len(y_pred)}")
    
    ss_res = np.sum((y_true - y_pred) ** 2)  # Residual sum of squares
    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)  # Total sum of squares
    
    if ss_tot == 0:
        return 0.0  # Táº¥t cáº£ y_true Ä‘á»u báº±ng nhau
    
    return float(1 - (ss_res / ss_tot))


def adjusted_r2_score(
    y_true: np.ndarray, 
    y_pred: np.ndarray, 
    n_features: int
) -> float:
    """
    Adjusted RÂ² - RÂ² Ä‘iá»u chá»‰nh theo sá»‘ features
    
    CÃ´ng thá»©c: Adj_RÂ² = 1 - [(1 - RÂ²) * (n - 1) / (n - p - 1)]
    
    Ã nghÄ©a:
    - Pháº¡t model cÃ³ quÃ¡ nhiá»u features
    - TrÃ¡nh overfitting do thÃªm feature vÃ´ nghÄ©a
    - NÃªn dÃ¹ng khi so sÃ¡nh models vá»›i sá»‘ features khÃ¡c nhau
    
    Args:
        y_true: GiÃ¡ trá»‹ thá»±c táº¿
        y_pred: GiÃ¡ trá»‹ dá»± Ä‘oÃ¡n
        n_features: Sá»‘ lÆ°á»£ng features trong model
        
    Returns:
        float: GiÃ¡ trá»‹ Adjusted RÂ²
    """
    r2 = r2_score(y_true, y_pred)
    n = len(y_true)
    
    if n <= n_features + 1:
        raise ValueError(
            f"Sá»‘ samples ({n}) pháº£i lá»›n hÆ¡n sá»‘ features + 1 ({n_features + 1})"
        )
    
    adj_r2 = 1 - ((1 - r2) * (n - 1) / (n - n_features - 1))
    return float(adj_r2)


# =============================================================================
# ğŸŒ§ï¸ WEATHER-SPECIFIC METRICS - Metrics Ä‘áº·c thÃ¹ cho dá»± bÃ¡o thá»i tiáº¿t
# =============================================================================

def rainfall_accuracy(
    y_true: np.ndarray, 
    y_pred: np.ndarray,
    threshold: float = 0.1
) -> float:
    """
    Accuracy cho dá»± bÃ¡o mÆ°a/khÃ´ng mÆ°a (binary classification tá»« regression)
    
    Ã nghÄ©a:
    - Chuyá»ƒn Ä‘á»•i lÆ°á»£ng mÆ°a thÃ nh cÃ³ mÆ°a (>threshold) / khÃ´ng mÆ°a
    - TÃ­nh accuracy cá»§a viá»‡c dá»± Ä‘oÃ¡n cÃ³ mÆ°a hay khÃ´ng
    
    Args:
        y_true: LÆ°á»£ng mÆ°a thá»±c táº¿ (mm)
        y_pred: LÆ°á»£ng mÆ°a dá»± Ä‘oÃ¡n (mm)
        threshold: NgÆ°á»¡ng xÃ¡c Ä‘á»‹nh cÃ³ mÆ°a (máº·c Ä‘á»‹nh 0.1mm)
        
    Returns:
        float: Accuracy (0-1)
    """
    y_true = np.asarray(y_true).flatten()
    y_pred = np.asarray(y_pred).flatten()
    
    # Chuyá»ƒn thÃ nh binary: cÃ³ mÆ°a = 1, khÃ´ng mÆ°a = 0
    true_rain = (y_true > threshold).astype(int)
    pred_rain = (y_pred > threshold).astype(int)
    
    accuracy = np.mean(true_rain == pred_rain)
    return float(accuracy)


def rainfall_precision_recall(
    y_true: np.ndarray, 
    y_pred: np.ndarray,
    threshold: float = 0.1
) -> Dict[str, float]:
    """
    Precision vÃ  Recall cho dá»± bÃ¡o mÆ°a
    
    Returns:
        Dict vá»›i keys: precision, recall, f1_score
    """
    y_true = np.asarray(y_true).flatten()
    y_pred = np.asarray(y_pred).flatten()
    
    true_rain = (y_true > threshold).astype(int)
    pred_rain = (y_pred > threshold).astype(int)
    
    # True Positives, False Positives, False Negatives
    tp = np.sum((true_rain == 1) & (pred_rain == 1))
    fp = np.sum((true_rain == 0) & (pred_rain == 1))
    fn = np.sum((true_rain == 1) & (pred_rain == 0))
    
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
    
    return {
        "precision": float(precision),
        "recall": float(recall),
        "f1_score": float(f1)
    }


def critical_success_index(
    y_true: np.ndarray, 
    y_pred: np.ndarray,
    threshold: float = 0.1
) -> float:
    """
    CSI - Critical Success Index (Threat Score)
    
    CÃ´ng thá»©c: CSI = TP / (TP + FP + FN)
    
    Ã nghÄ©a:
    - Metric phá»• biáº¿n trong dá»± bÃ¡o thá»i tiáº¿t
    - ÄÃ¡nh giÃ¡ kháº£ nÄƒng dá»± bÃ¡o sá»± kiá»‡n (mÆ°a)
    - KhÃ´ng tÃ­nh True Negatives (ngÃ y khÃ´ng mÆ°a Ä‘Ãºng)
    
    Args:
        y_true: LÆ°á»£ng mÆ°a thá»±c táº¿
        y_pred: LÆ°á»£ng mÆ°a dá»± Ä‘oÃ¡n
        threshold: NgÆ°á»¡ng xÃ¡c Ä‘á»‹nh cÃ³ mÆ°a
        
    Returns:
        float: CSI score (0-1)
    """
    y_true = np.asarray(y_true).flatten()
    y_pred = np.asarray(y_pred).flatten()
    
    true_rain = (y_true > threshold).astype(int)
    pred_rain = (y_pred > threshold).astype(int)
    
    tp = np.sum((true_rain == 1) & (pred_rain == 1))
    fp = np.sum((true_rain == 0) & (pred_rain == 1))
    fn = np.sum((true_rain == 1) & (pred_rain == 0))
    
    denominator = tp + fp + fn
    if denominator == 0:
        return 0.0
    
    return float(tp / denominator)


def bias_score(
    y_true: np.ndarray, 
    y_pred: np.ndarray,
    threshold: float = 0.1
) -> float:
    """
    Bias Score (Frequency Bias Index)
    
    CÃ´ng thá»©c: Bias = (TP + FP) / (TP + FN)
    
    Ã nghÄ©a:
    - Bias = 1: Model khÃ´ng thiÃªn lá»‡ch
    - Bias > 1: Model dá»± bÃ¡o mÆ°a quÃ¡ nhiá»u (over-forecast)
    - Bias < 1: Model dá»± bÃ¡o mÆ°a quÃ¡ Ã­t (under-forecast)
    
    Args:
        y_true: LÆ°á»£ng mÆ°a thá»±c táº¿
        y_pred: LÆ°á»£ng mÆ°a dá»± Ä‘oÃ¡n
        threshold: NgÆ°á»¡ng xÃ¡c Ä‘á»‹nh cÃ³ mÆ°a
        
    Returns:
        float: Bias score
    """
    y_true = np.asarray(y_true).flatten()
    y_pred = np.asarray(y_pred).flatten()
    
    true_rain = (y_true > threshold).astype(int)
    pred_rain = (y_pred > threshold).astype(int)
    
    tp = np.sum((true_rain == 1) & (pred_rain == 1))
    fp = np.sum((true_rain == 0) & (pred_rain == 1))
    fn = np.sum((true_rain == 1) & (pred_rain == 0))
    
    denominator = tp + fn
    if denominator == 0:
        return 0.0
    
    return float((tp + fp) / denominator)


# =============================================================================
# ğŸ”§ UTILITY FUNCTIONS - HÃ m tiá»‡n Ã­ch
# =============================================================================

def calculate_all_metrics(
    y_true: np.ndarray, 
    y_pred: np.ndarray,
    n_features: Optional[int] = None,
    include_weather_metrics: bool = False,
    rain_threshold: float = 0.1
) -> Dict[str, float]:
    """
    TÃ­nh táº¥t cáº£ metrics má»™t láº§n
    
    Args:
        y_true: GiÃ¡ trá»‹ thá»±c táº¿
        y_pred: GiÃ¡ trá»‹ dá»± Ä‘oÃ¡n
        n_features: Sá»‘ features (Ä‘á»ƒ tÃ­nh Adjusted RÂ²)
        include_weather_metrics: CÃ³ tÃ­nh metrics Ä‘áº·c thÃ¹ thá»i tiáº¿t khÃ´ng
        rain_threshold: NgÆ°á»¡ng mÆ°a (náº¿u include_weather_metrics=True)
        
    Returns:
        Dict chá»©a táº¥t cáº£ metrics
    """
    results = {
        "MAE": mean_absolute_error(y_true, y_pred),
        "MSE": mean_squared_error(y_true, y_pred),
        "RMSE": root_mean_squared_error(y_true, y_pred),
        "MAPE": mean_absolute_percentage_error(y_true, y_pred),
        "sMAPE": symmetric_mean_absolute_percentage_error(y_true, y_pred),
        "R2": r2_score(y_true, y_pred),
    }
    
    # Adjusted RÂ² náº¿u cÃ³ sá»‘ features
    if n_features is not None:
        try:
            results["Adjusted_R2"] = adjusted_r2_score(y_true, y_pred, n_features)
        except ValueError:
            results["Adjusted_R2"] = None
    
    # Weather-specific metrics
    if include_weather_metrics:
        results["Rain_Accuracy"] = rainfall_accuracy(y_true, y_pred, rain_threshold)
        results["CSI"] = critical_success_index(y_true, y_pred, rain_threshold)
        results["Bias"] = bias_score(y_true, y_pred, rain_threshold)
        
        pr_metrics = rainfall_precision_recall(y_true, y_pred, rain_threshold)
        results["Rain_Precision"] = pr_metrics["precision"]
        results["Rain_Recall"] = pr_metrics["recall"]
        results["Rain_F1"] = pr_metrics["f1_score"]
    
    return results


def compare_models(
    y_true: np.ndarray,
    predictions: Dict[str, np.ndarray],
    primary_metric: str = "RMSE"
) -> Dict[str, Dict]:
    """
    So sÃ¡nh nhiá»u models vá»›i nhau
    
    Args:
        y_true: GiÃ¡ trá»‹ thá»±c táº¿
        predictions: Dict {model_name: y_pred}
        primary_metric: Metric chÃ­nh Ä‘á»ƒ xáº¿p háº¡ng
        
    Returns:
        Dict vá»›i metrics cá»§a tá»«ng model + ranking
    """
    results = {}
    
    for model_name, y_pred in predictions.items():
        metrics = calculate_all_metrics(y_true, y_pred)
        results[model_name] = metrics
    
    # Xáº¿p háº¡ng theo primary_metric
    # (lower is better for MAE/MSE/RMSE/MAPE, higher is better for RÂ²)
    lower_is_better = primary_metric in ["MAE", "MSE", "RMSE", "MAPE", "sMAPE"]
    
    sorted_models = sorted(
        results.keys(),
        key=lambda m: results[m].get(primary_metric, float('inf')),
        reverse=not lower_is_better
    )
    
    for rank, model_name in enumerate(sorted_models, 1):
        results[model_name]["rank"] = rank
    
    return results


def format_metrics_report(metrics: Dict[str, float]) -> str:
    """
    Format metrics thÃ nh string Ä‘áº¹p Ä‘á»ƒ in ra console/log
    
    Args:
        metrics: Dict chá»©a cÃ¡c metrics
        
    Returns:
        String formatted
    """
    lines = [
        "=" * 50,
        "ğŸ“Š EVALUATION METRICS REPORT",
        "=" * 50,
    ]
    
    # Group metrics
    regression_metrics = ["MAE", "MSE", "RMSE", "MAPE", "sMAPE", "R2", "Adjusted_R2"]
    weather_metrics = ["Rain_Accuracy", "CSI", "Bias", "Rain_Precision", "Rain_Recall", "Rain_F1"]
    
    lines.append("\nğŸ“ Regression Metrics:")
    lines.append("-" * 30)
    for m in regression_metrics:
        if m in metrics and metrics[m] is not None:
            value = metrics[m]
            if m in ["MAPE", "sMAPE"]:
                lines.append(f"  {m:15} : {value:>10.4f} %")
            else:
                lines.append(f"  {m:15} : {value:>10.6f}")
    
    # Weather metrics náº¿u cÃ³
    weather_present = any(m in metrics for m in weather_metrics)
    if weather_present:
        lines.append("\nğŸŒ§ï¸ Weather-Specific Metrics:")
        lines.append("-" * 30)
        for m in weather_metrics:
            if m in metrics and metrics[m] is not None:
                value = metrics[m]
                if "Accuracy" in m or "Precision" in m or "Recall" in m or "F1" in m:
                    lines.append(f"  {m:15} : {value:>10.4f} ({value*100:.2f}%)")
                else:
                    lines.append(f"  {m:15} : {value:>10.4f}")
    
    lines.append("=" * 50)
    
    return "\n".join(lines)


# =============================================================================
# ğŸ§ª SIMPLE TEST / DEMO
# =============================================================================

if __name__ == "__main__":
    # Demo vá»›i dá»¯ liá»‡u giáº£
    np.random.seed(42)
    
    # Giáº£ láº­p dá»¯ liá»‡u lÆ°á»£ng mÆ°a (mm)
    y_true = np.array([0.0, 0.0, 5.2, 12.3, 0.0, 8.7, 25.1, 0.0, 3.4, 15.8])
    y_pred = np.array([0.5, 0.0, 4.8, 10.1, 0.2, 9.5, 22.0, 0.0, 4.1, 14.2])
    
    print("\nğŸŒ§ï¸ Demo: ÄÃ¡nh giÃ¡ dá»± bÃ¡o lÆ°á»£ng mÆ°a")
    print("=" * 50)
    print(f"y_true: {y_true}")
    print(f"y_pred: {y_pred}")
    
    # TÃ­nh táº¥t cáº£ metrics
    all_metrics = calculate_all_metrics(
        y_true, y_pred, 
        n_features=5,
        include_weather_metrics=True
    )
    
    # In report
    print(format_metrics_report(all_metrics))
    
    # So sÃ¡nh nhiá»u models
    print("\nğŸ“Š So sÃ¡nh 3 models:")
    predictions = {
        "XGBoost": y_pred,
        "LightGBM": y_pred + np.random.randn(10) * 0.5,
        "CatBoost": y_pred - np.random.randn(10) * 0.3,
    }
    
    comparison = compare_models(y_true, predictions, primary_metric="RMSE")
    
    for model, metrics in comparison.items():
        print(f"\nğŸ”¹ {model} (Rank #{metrics['rank']}):")
        print(f"   RMSE: {metrics['RMSE']:.4f}")
        print(f"   MAE:  {metrics['MAE']:.4f}")
        print(f"   RÂ²:   {metrics['R2']:.4f}")
