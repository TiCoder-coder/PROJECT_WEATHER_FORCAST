<div align="center">

# ğŸŒ¿ LightGBM â€” Light Gradient Boosting Machine  
<sub><b>GBDT siÃªu nhanh cho dá»¯ liá»‡u dáº¡ng báº£ng (tabular)</b> â€” phÃ¢n loáº¡i, há»“i quy, ranking</sub>

<br/>

<img alt="LightGBM" src="https://img.shields.io/badge/Model-Gradient%20Boosting%20Trees-2ea44f?style=for-the-badge" />
<img alt="Use-case" src="https://img.shields.io/badge/Use--case-Tabular%20Data-blue?style=for-the-badge" />
<img alt="Focus" src="https://img.shields.io/badge/Focus-Speed%20%2B%20Memory-orange?style=for-the-badge" />

<br/><br/>

</div>

---

## ğŸ“š Má»¥c lá»¥c
- [1. LightGBM lÃ  gÃ¬?](#1-lightgbm-lÃ -gÃ¬)
- [2. Táº¡i sao LightGBM nhanh? (Ã½ tÆ°á»Ÿng cá»‘t lÃµi)](#2-táº¡i-sao-lightgbm-nhanh-Ã½-tÆ°á»Ÿng-cá»‘t-lÃµi)
- [3. LightGBM lÃ m Ä‘Æ°á»£c gÃ¬? (bÃ i toÃ¡n & objective)](#3-lightgbm-lÃ m-Ä‘Æ°á»£c-gÃ¬-bÃ i-toÃ¡n--objective)
- [4. CÃ¡c Ä‘áº·c Ä‘iá»ƒm ná»•i báº­t](#4-cÃ¡c-Ä‘áº·c-Ä‘iá»ƒm-ná»•i-báº­t)
- [5. Tham sá»‘ quan trá»ng (cheat sheet)](#5-tham-sá»‘-quan-trá»ng-cheat-sheet)
- [6. Quy trÃ¬nh train chuáº©n (thá»±c chiáº¿n)](#6-quy-trÃ¬nh-train-chuáº©n-thá»±c-chiáº¿n)
- [7. Categorical & Missing Values: lÃ m Ä‘Ãºng ngay tá»« Ä‘áº§u](#7-categorical--missing-values-lÃ m-Ä‘Ãºng-ngay-tá»«-Ä‘áº§u)
- [8. VÃ­ dá»¥ code nhanh (Python)](#8-vÃ­-dá»¥-code-nhanh-python)
- [9. Diá»…n giáº£i mÃ´ hÃ¬nh (interpretability)](#9-diá»…n-giáº£i-mÃ´-hÃ¬nh-interpretability)
- [10. Æ¯u & nhÆ°á»£c Ä‘iá»ƒm](#10-Æ°u--nhÆ°á»£c-Ä‘iá»ƒm)
- [11. So sÃ¡nh LightGBM vs XGBoost](#11-so-sÃ¡nh-lightgbm-vs-xgboost)
- [12. Nhá»¯ng â€œbáº«yâ€ hay gáº·p & checklist debug](#12-nhá»¯ng-báº«y-hay-gáº·p--checklist-debug)
- [13. TÃ i liá»‡u tham kháº£o](#13-tÃ i-liá»‡u-tham-kháº£o)

---

## 1. LightGBM lÃ  gÃ¬?

**LightGBM (Light Gradient Boosting Machine)** lÃ  má»™t framework thuá»™c há» **Gradient Boosting Decision Trees (GBDT)**.  NÃ³ xÃ¢y dá»±ng mÃ´ hÃ¬nh báº±ng cÃ¡ch **cá»™ng dá»“n nhiá»u cÃ¢y quyáº¿t Ä‘á»‹nh** (decision trees) theo kiá»ƒu *boosting*: má»—i cÃ¢y má»›i cá»‘ gáº¯ng **sá»­a lá»—i** (giáº£m loss) mÃ  cÃ¡c cÃ¢y trÆ°á»›c cÃ²n máº¯c pháº£i.

> âœ… LightGBM ná»•i tiáº¿ng vÃ¬: **train nhanh**, **tá»‘n Ã­t RAM**, **scale tá»‘t** cho dá»¯ liá»‡u lá»›n / nhiá»u feature, vÃ  váº«n cho cháº¥t lÆ°á»£ng mÃ´ hÃ¬nh ráº¥t máº¡nh trÃªn dá»¯ liá»‡u dáº¡ng báº£ng.

---

## 2. Táº¡i sao LightGBM nhanh? (Ã½ tÆ°á»Ÿng cá»‘t lÃµi)

### 2.1 Histogram-based split (chia ngÆ°á»¡ng theo histogram)
- Thay vÃ¬ thá»­ má»i giÃ¡ trá»‹ liÃªn tá»¥c Ä‘á»ƒ tÃ¬m split tá»‘t nháº¥t, LightGBM **bucket hÃ³a** giÃ¡ trá»‹ feature thÃ nh cÃ¡c **bins** (histogram).  
â¡ï¸ Giáº£m ráº¥t máº¡nh sá»‘ phÃ©p tÃ­nh vÃ  giáº£m bá»™ nhá»›.

**Hiá»‡u quáº£ thá»±c táº¿:**
- Dá»¯ liá»‡u cÃ ng lá»›n â†’ cÃ ng tháº¥y lá»£i tháº¿ rÃµ.
- CÃ³ thá»ƒ Ä‘iá»u chá»‰nh `max_bin` Ä‘á»ƒ trade-off **nhanh** vs **chÃ­nh xÃ¡c**.

---

### 2.2 Leaf-wise (best-first) tree growth â€” Ä‘iá»ƒm khÃ¡c biá»‡t lá»›n
Nhiá»u GBDT grow cÃ¢y theo kiá»ƒu **level-wise** (tÄƒng theo táº§ng, cÃ¢y cÃ¢n Ä‘á»‘i).  
LightGBM grow kiá»ƒu **leaf-wise**: **luÃ´n split lÃ¡ nÃ o giáº£m loss nhiá»u nháº¥t trÆ°á»›c**.

**Æ¯u Ä‘iá»ƒm:** thÆ°á»ng cho accuracy tá»‘t hÆ¡n vá»›i cÃ¹ng sá»‘ split.  
**NhÆ°á»£c Ä‘iá»ƒm:** **dá»… overfit** náº¿u khÃ´ng giá»›i háº¡n Ä‘á»™ phá»©c táº¡p (vÃ¬ cÃ¢y cÃ³ thá»ƒ ráº¥t â€œsÃ¢uâ€ á»Ÿ má»™t nhÃ¡nh).

ğŸ‘‰ VÃ¬ váº­y trong tune, báº¡n gáº§n nhÆ° luÃ´n pháº£i Ä‘á»ƒ Ã½: `num_leaves`, `max_depth`, `min_data_in_leaf`.

âœ¨ Má»™t sá»‘ chá»‰ sá»‘ cáº§n nhá»› khi lÃ m viá»‡c vá»›i LightGBM:

- **objective**: Tham sá»‘ nÃ y dÃ¹ng Ä‘á»ƒ xÃ¡c Ä‘á»‹nh má»¥c tiÃªu cá»§a bÃ i toÃ¡n mÃ  báº¡n Ä‘ang cá»‘ gáº¯ng giáº£i quyáº¿t. Nhá»¯ng giÃ¡ trá»‹ thÆ°á»ng gáº·p gá»“m cÃ³ â€˜binaryâ€™ (cho cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i chá»‰ cÃ³ hai lá»›p), â€˜multiclassâ€™ (cho bÃ i toÃ¡n phÃ¢n loáº¡i cÃ³ nhiá»u hÆ¡n hai lá»›p), vÃ  â€˜regressionâ€™ (cho cÃ¡c bÃ i toÃ¡n dá»± Ä‘oÃ¡n giÃ¡ trá»‹ liÃªn tá»¥c). ---------- VN WEATHER HUUB ÄANG Sá»¬ Dá»¤NG "REGESTION" VÃŒ Dá»° BÃO THá»œI TIáº¾T.

- **metric**: ÄÆ°á»£c sá»­ dá»¥ng Ä‘á»ƒ chá»‰ Ä‘á»‹nh thÆ°á»›c Ä‘o (chá»‰ sá»‘) báº¡n muá»‘n dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng cá»§a mÃ´ hÃ¬nh trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n hoáº·c kiá»ƒm thá»­. VÃ­ dá»¥, báº¡n cÃ³ thá»ƒ chá»n â€˜binary_errorâ€™ khi lÃ m viá»‡c vá»›i bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n.

- **num_leaves**: Tham sá»‘ nÃ y quy Ä‘á»‹nh sá»‘ lÆ°á»£ng lÃ¡ tá»‘i Ä‘a cho má»—i cÃ¢y quyáº¿t Ä‘á»‹nh Ä‘Æ°á»£c táº¡o ra. NÃ³ lÃ  má»™t yáº¿u tá»‘ quan trá»ng áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n má»©c Ä‘á»™ phá»©c táº¡p (complexity) cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n.

- **learning_rate**: CÃ²n gá»i lÃ  tá»‘c Ä‘á»™ há»c, tham sá»‘ nÃ y kiá»ƒm soÃ¡t má»©c Ä‘á»™ Ä‘iá»u chá»‰nh trá»ng sá»‘ cá»§a mÃ´ hÃ¬nh sau má»—i vÃ²ng láº·p boosting, qua Ä‘Ã³ tÃ¡c Ä‘á»™ng Ä‘áº¿n tá»‘c Ä‘á»™ há»™i tá»¥ cá»§a mÃ´ hÃ¬nh (há»c nhanh hay cháº­m).

- **feature_fraction**: Tham sá»‘ nÃ y xÃ¡c Ä‘á»‹nh tá»· lá»‡ (pháº§n trÄƒm) cÃ¡c Ä‘áº·c trÆ°ng sáº½ Ä‘Æ°á»£c lá»±a chá»n ngáº«u nhiÃªn Ä‘á»ƒ sá»­ dá»¥ng trong quÃ¡ trÃ¬nh xÃ¢y dá»±ng má»—i cÃ¢y quyáº¿t Ä‘á»‹nh riÃªng láº».

- **bagging_fraction** vÃ  **bagging_freq**: Bá»™ Ä‘Ã´i tham sá»‘ nÃ y cho phÃ©p báº¡n kÃ­ch hoáº¡t ká»¹ thuáº­t bagging. bagging_fraction quy Ä‘á»‹nh tá»· lá»‡ máº«u dá»¯ liá»‡u huáº¥n luyá»‡n Ä‘Æ°á»£c chá»n ngáº«u nhiÃªn (cÃ³ láº·p láº¡i) cho má»—i cÃ¢y, vÃ  bagging_freq xÃ¡c Ä‘á»‹nh táº§n suáº¥t thá»±c hiá»‡n bagging (vÃ­ dá»¥: thá»±c hiá»‡n bagging sau má»—i k vÃ²ng láº·p). Má»¥c Ä‘Ã­ch chÃ­nh cá»§a bagging lÃ  giÃºp mÃ´ hÃ¬nh giáº£m thiá»ƒu hiá»‡n tÆ°á»£ng quÃ¡ khá»›p (overfitting).

---

### 2.3 GOSS â€” Gradient-based One-Side Sampling
**GOSS** láº¥y máº«u thÃ´ng minh:
- giá»¯ nhiá»u Ä‘iá»ƒm cÃ³ **gradient lá»›n** (Ä‘iá»ƒm â€œkhÃ³â€, áº£nh hÆ°á»Ÿng lá»›n Ä‘áº¿n loss),
- sampling pháº§n gradient nhá».

â¡ï¸ Má»¥c tiÃªu: giáº£m dá»¯ liá»‡u cáº§n tÃ­nh split nhÆ°ng váº«n giá»¯ â€œthÃ´ng tin quan trá»ngâ€ cho boosting.

---

### 2.4 EFB â€” Exclusive Feature Bundling
**EFB** gá»™p cÃ¡c feature â€œhiáº¿m khi cÃ¹ng khÃ¡c 0â€ (thÆ°á»ng gáº·p á»Ÿ dá»¯ liá»‡u sparse / one-hot / text-like).  
â¡ï¸ Giáº£m sá»‘ chiá»u hiá»‡u quáº£ â†’ giáº£m chi phÃ­ tÃ­nh histogram/split.

---

## 3. LightGBM lÃ m Ä‘Æ°á»£c gÃ¬? (bÃ i toÃ¡n & objective)

LightGBM thÆ°á»ng dÃ¹ng cho:

- âœ… **Classification**: nhá»‹ phÃ¢n (`binary`), Ä‘a lá»›p (`multiclass`)
- âœ… **Regression**: dá»± Ä‘oÃ¡n sá»‘ thá»±c (`regression`, `regression_l1`, â€¦)
- âœ… **Ranking**: xáº¿p háº¡ng (trong search/recommendation) (`lambdarank`)
- âœ… **Quantile / Poisson / Tweedie**â€¦ (dá»¯ liá»‡u Ä‘áº·c thÃ¹)

---

## 4. CÃ¡c Ä‘áº·c Ä‘iá»ƒm ná»•i báº­t

- âš¡ **Hiá»‡u nÄƒng vÆ°á»£t trá»™i**: train nhanh, Ã­t RAM (Ä‘áº·c biá»‡t vá»›i histogram + EFB).
- ğŸ§± **Xá»­ lÃ½ dá»¯ liá»‡u lá»›n**: nhiá»u máº«u & nhiá»u feature.
- ğŸ§  **Äa nÄƒng**: phÃ¢n loáº¡i + há»“i quy + ranking.
- ğŸ§µ **Há»— trá»£ song song & phÃ¢n tÃ¡n**: multi-core CPU, training phÃ¢n tÃ¡n (tuá»³ setup).
- ğŸ›ï¸ **Linh hoáº¡t tham sá»‘**: tune sÃ¢u Ä‘á»ƒ tá»‘i Æ°u theo tá»«ng bÃ i toÃ¡n.
- ğŸ•³ï¸ **Xá»­ lÃ½ missing value tá»‘t**: nhiá»u trÆ°á»ng há»£p khÃ´ng cáº§n impute phá»©c táº¡p.
- ğŸ·ï¸ **Há»— trá»£ categorical**: náº¿u khai bÃ¡o Ä‘Ãºng kiá»ƒu/cá»™t categorical.

---

## 5. Tham sá»‘ quan trá»ng (cheat sheet)

> Náº¿u pháº£i nhá»› **5 tham sá»‘ quan trá»ng nháº¥t**:  
> `learning_rate`, `n_estimators`, `num_leaves`, `min_data_in_leaf`, `feature_fraction/bagging_fraction`

### 5.1 NhÃ³m má»¥c tiÃªu & metric
- `objective`: má»¥c tiÃªu bÃ i toÃ¡n  
  - `binary`, `multiclass`, `regression`, â€¦
- `metric`: thÆ°á»›c Ä‘o trong training/validation  
  - `auc`, `binary_logloss`, `rmse`, `mae`, â€¦

### 5.2 NhÃ³m sá»‘ cÃ¢y & tá»‘c Ä‘á»™ há»c
- `n_estimators` / `num_boost_round`: sá»‘ cÃ¢y
- `learning_rate`: tá»‘c Ä‘á»™ há»c  
  - nhá» hÆ¡n â†’ cáº§n nhiá»u cÃ¢y hÆ¡n, thÆ°á»ng á»•n Ä‘á»‹nh hÆ¡n

### 5.3 NhÃ³m Ä‘á»™ phá»©c táº¡p cÃ¢y (cá»±c quan trá»ng vÃ¬ leaf-wise)
- `num_leaves`: sá»‘ lÃ¡ tá»‘i Ä‘a má»—i cÃ¢y (**top 1**)  
- `max_depth`: giá»›i háº¡n Ä‘á»™ sÃ¢u (giáº£m overfit)
- `min_data_in_leaf` (aka `min_child_samples`): tá»‘i thiá»ƒu máº«u trong 1 lÃ¡  
- `min_sum_hessian_in_leaf`: lÃ m lÃ¡ á»•n Ä‘á»‹nh hÆ¡n (nháº¥t lÃ  dá»¯ liá»‡u nhiá»…u)

### 5.4 NhÃ³m sampling Ä‘á»ƒ giáº£m overfit
- `feature_fraction` (aka `colsample_bytree`): % feature má»—i cÃ¢y
- `bagging_fraction` (aka `subsample`): % sample má»—i cÃ¢y
- `bagging_freq` (aka `subsample_freq`): táº§n suáº¥t bagging

### 5.5 NhÃ³m regularization
- `lambda_l1` (aka `reg_alpha`)
- `lambda_l2` (aka `reg_lambda`)
- `min_gain_to_split`: gain tá»‘i thiá»ƒu má»›i Ä‘Æ°á»£c split

### 5.6 NhÃ³m histogram
- `max_bin`: sá»‘ bins cho histogram (nhá» hÆ¡n â†’ nhanh hÆ¡n, cÃ³ thá»ƒ giáº£m accuracy)

---

## 6. Quy trÃ¬nh train chuáº©n (thá»±c chiáº¿n)

### 6.1 â€œRecipeâ€ nhanh cho Ä‘a sá»‘ bÃ i tabular
1. Chia `train/valid` chuáº©n (hoáº·c K-fold CV).  
2. Train vá»›i `early_stopping` Ä‘á»ƒ tÃ¬m sá»‘ cÃ¢y tá»‘i Æ°u.  
3. Tune theo thá»© tá»±:
   - **(A) Tree complexity:** `num_leaves`, `max_depth`, `min_data_in_leaf`
   - **(B) Sampling:** `feature_fraction`, `bagging_fraction`, `bagging_freq`
   - **(C) Regularization + histogram:** `lambda_l1/l2`, `min_gain_to_split`, `max_bin`
4. Chá»‘t láº¡i báº±ng CV, rá»“i train full train vá»›i `best_iteration`.

### 6.2 Quy táº¯c trÃ¡nh overfit (ráº¥t hay gáº·p)
- Náº¿u **train ráº¥t tá»‘t, valid tá»‡**:
  - giáº£m `num_leaves`
  - tÄƒng `min_data_in_leaf`
  - thÃªm sampling (`feature_fraction`, `bagging_fraction`)
  - tÄƒng `lambda_l2` (hoáº·c `lambda_l1`)
  - giá»›i háº¡n `max_depth`

---

## 7. Categorical & Missing Values: lÃ m Ä‘Ãºng ngay tá»« Ä‘áº§u

### 7.1 Missing values
- LightGBM thÆ°á»ng xá»­ lÃ½ missing trá»±c tiáº¿p khÃ¡ tá»‘t.
- Tuy nhiÃªn, náº¿u missing mang Ã½ nghÄ©a riÃªng, báº¡n cÃ³ thá»ƒ thÃªm feature `is_missing`.

### 7.2 Categorical â€” lá»—i hay gáº·p nháº¥t
âœ… NÃªn:
- chuyá»ƒn cá»™t categorical vá» kiá»ƒu `category` (pandas) hoáº·c integer-coded,
- khai bÃ¡o `categorical_feature`.

âŒ TrÃ¡nh:
- label-encode xong **nhÆ°ng quÃªn set categorical** â†’ model coi nhÆ° biáº¿n sá»‘ liÃªn tá»¥c (split numeric) â†’ sai báº£n cháº¥t.

> Vá»›i categorical cÃ³ **cardinality ráº¥t cao**: cÃ¢n nháº¯c gá»™p rare categories / hashing / target encoding (cáº©n tháº­n leakage).

---

## 8. VÃ­ dá»¥ code nhanh (Python)

### 8.1 CÃ i Ä‘áº·t
```bash
pip install lightgbm
# hoáº·c
conda install -c conda-forge lightgbm
```

### 8.2 Sklearn API â€” Classification (nhá»‹ phÃ¢n)
```python
import lightgbm as lgb
from lightgbm import LGBMClassifier

model = LGBMClassifier(
    n_estimators=5000,
    learning_rate=0.05,
    num_leaves=64,
    max_depth=-1,
    min_child_samples=30,
    subsample=0.8,
    subsample_freq=1,
    colsample_bytree=0.8,
    reg_alpha=0.0,
    reg_lambda=0.0,
    random_state=42,
)

model.fit(
    X_train, y_train,
    eval_set=[(X_valid, y_valid)],
    eval_metric="auc",
    callbacks=[lgb.early_stopping(stopping_rounds=200)]
)

print("best_iteration =", model.best_iteration_)
```

### 8.3 Regression
```python
from lightgbm import LGBMRegressor
import lightgbm as lgb

reg = LGBMRegressor(
    n_estimators=10000,
    learning_rate=0.03,
    num_leaves=64,
    min_child_samples=40,
    subsample=0.8,
    subsample_freq=1,
    colsample_bytree=0.8,
    random_state=42,
)

reg.fit(
    X_train, y_train,
    eval_set=[(X_valid, y_valid)],
    eval_metric="rmse",
    callbacks=[lgb.early_stopping(300)]
)
```

### 8.4 Categorical Ä‘Ãºng cÃ¡ch (pandas)
```python
cat_cols = ["city", "channel", "product_type"]

for c in cat_cols:
    X_train[c] = X_train[c].astype("category")
    X_valid[c] = X_valid[c].astype("category")

model = LGBMClassifier(
    n_estimators=5000,
    learning_rate=0.05,
    num_leaves=64,
    random_state=42,
)

model.fit(
    X_train, y_train,
    eval_set=[(X_valid, y_valid)],
    categorical_feature=cat_cols,
    callbacks=[lgb.early_stopping(200)]
)
```

### 8.5 Native LightGBM API (Dataset + train)
```python
import lightgbm as lgb

train_data = lgb.Dataset(X_train, label=y_train)
valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)

params = {
    "objective": "binary",
    "metric": "auc",
    "learning_rate": 0.05,
    "num_leaves": 64,
    "feature_fraction": 0.8,
    "bagging_fraction": 0.8,
    "bagging_freq": 1,
    "min_data_in_leaf": 30,
}

bst = lgb.train(
    params,
    train_data,
    num_boost_round=5000,
    valid_sets=[valid_data],
    callbacks=[lgb.early_stopping(200)]
)

print("best_iteration =", bst.best_iteration)
```

---

## 9. Diá»…n giáº£i mÃ´ hÃ¬nh (interpretability)

### 9.1 Feature importance
- `gain`: tá»•ng gain do feature táº¡o ra  
- `split`: sá»‘ láº§n feature Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ split

Trong sklearn API:
```python
import pandas as pd

imp = pd.DataFrame({
    "feature": X_train.columns,
    "importance": model.feature_importances_,
}).sort_values("importance", ascending=False)
print(imp.head(20))
```

### 9.2 SHAP (giáº£i thÃ­ch theo tá»«ng dá»± Ä‘oÃ¡n)
- SHAP ráº¥t há»¯u Ã­ch Ä‘á»ƒ hiá»ƒu â€œvÃ¬ sao model dá»± Ä‘oÃ¡n nhÆ° váº­yâ€
- Cáº©n tháº­n vá»›i dá»¯ liá»‡u lá»›n: SHAP cÃ³ thá»ƒ tá»‘n thá»i gian

```python
# pip install shap
import shap
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_valid)
```

---

## 10. Æ¯u & nhÆ°á»£c Ä‘iá»ƒm

### âœ… Æ¯u Ä‘iá»ƒm
- âš¡ **Nhanh & tiáº¿t kiá»‡m bá»™ nhá»›** (ráº¥t máº¡nh vá»›i data lá»›n): LightGBM ná»•i báº­t vá»›i kháº£ nÄƒng xá»­ lÃ½ nhanh chÃ³ng vÃ  hiá»‡u quáº£ cÃ¡c táº­p dá»¯ liá»‡u cÃ³ dung lÆ°á»£ng lá»›n. Nhá» Ã¡p dá»¥ng cÃ¡c thuáº­t toÃ¡n Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a, LightGBM yÃªu cáº§u Ã­t bá»™ nhá»› hÆ¡n so vá»›i khi sá»­ dá»¥ng cÃ¡c thuáº­t toÃ¡n boosting tháº¿ há»‡ trÆ°á»›c.
- ğŸ§µ **Song song tá»‘t** trÃªn CPU: LightGBM cÃ³ thá»ƒ rÃºt ngáº¯n Ä‘Ã¡ng ká»ƒ thá»i gian huáº¥n luyá»‡n mÃ´ hÃ¬nh thÃ´ng qua viá»‡c khai thÃ¡c hiá»‡u quáº£ kháº£ nÄƒng tÃ­nh toÃ¡n song song.
- ğŸ§  **Cháº¥t lÆ°á»£ng cao** trÃªn tabular
- ğŸ›ï¸ **Nhiá»u tham sá»‘** Ä‘á»ƒ tá»‘i Æ°u theo bÃ i toÃ¡n: : NgÆ°á»i dÃ¹ng cÃ³ Ä‘Æ°á»£c sá»± linh hoáº¡t cao do LightGBM cung cáº¥p má»™t loáº¡t cÃ¡c tham sá»‘ cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘iá»u chá»‰nh Ä‘á»ƒ tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh cho tá»«ng bÃ i toÃ¡n cá»¥ thá»ƒ.
- ğŸ·ï¸ **Há»— trá»£ categorical/missing** tá»‘t náº¿u khai bÃ¡o Ä‘Ãºng

### â— NhÆ°á»£c Ä‘iá»ƒm
- ğŸ§© **Dá»… overfit** náº¿u `num_leaves` lá»›n, thiáº¿u rÃ ng buá»™c (do leaf-wise)
- ğŸ› ï¸ **Cáº§n tune tham sá»‘** Ä‘á»ƒ Ä‘áº¡t hiá»‡u quáº£ tá»‘i Ä‘a
**==> ÄÃ²i há»i tinh chá»‰nh tham sá»‘ ká»¹ lÆ°á»¡ng: Äá»ƒ LightGBM hoáº¡t Ä‘á»™ng vá»›i hiá»‡u quáº£ cao nháº¥t, viá»‡c Ä‘iá»u chá»‰nh cáº©n tháº­n nhiá»u tham sá»‘ cáº¥u hÃ¬nh lÃ  má»™t yÃªu cáº§u cáº§n thiáº¿t.**
- ğŸ§  **Diá»…n giáº£i khÃ´ng trá»±c quan** vá»›i ngÆ°á»i má»›i (cáº§n tools nhÆ° SHAP): MÃ´ hÃ¬nh do LightGBM táº¡o ra cÃ³ thá»ƒ khÃ³ hiá»ƒu vÃ  diá»…n giáº£i, Ä‘áº·c biá»‡t Ä‘á»‘i vá»›i nhá»¯ng cÃ¡ nhÃ¢n má»›i báº¯t Ä‘áº§u tÃ¬m hiá»ƒu vá» lÄ©nh vá»±c há»c mÃ¡y.

---

## 11. So sÃ¡nh LightGBM vs XGBoost

<table>
  <tr>
    <th>TiÃªu chÃ­</th>
    <th>LightGBM</th>
    <th>XGBoost</th>
  </tr>
  <tr>
    <td><b>Tá»‘c Ä‘á»™ train</b></td>
    <td>ThÆ°á»ng <b>nhanh hÆ¡n</b> (histogram + EFB + leaf-wise)</td>
    <td>Nhanh, nhÆ°ng nhiá»u case cháº­m hÆ¡n LGBM</td>
  </tr>
  <tr>
    <td><b>RAM</b></td>
    <td>ThÆ°á»ng <b>tá»‘i Æ°u hÆ¡n</b> trÃªn dá»¯ liá»‡u lá»›n</td>
    <td>á»”n, nhÆ°ng cÃ³ thá»ƒ tá»‘n hÆ¡n trÃªn data ráº¥t lá»›n</td>
  </tr>
  <tr>
    <td><b>Chiáº¿n lÆ°á»£c grow cÃ¢y</b></td>
    <td><b>Leaf-wise</b> (best-first) â†’ máº¡nh nhÆ°ng dá»… overfit</td>
    <td>ThÆ°á»ng level-wise / controlled â†’ á»•n Ä‘á»‹nh hÆ¡n</td>
  </tr>
  <tr>
    <td><b>Dá»¯ liá»‡u nhá»</b></td>
    <td>CÃ³ thá»ƒ overfit náº¿u tune chÆ°a tá»‘t</td>
    <td>ThÆ°á»ng á»•n Ä‘á»‹nh hÆ¡n</td>
  </tr>
  <tr>
    <td><b>Categorical</b></td>
    <td>CÃ³ há»— trá»£ (cáº§n khai bÃ¡o Ä‘Ãºng)</td>
    <td>Há»— trá»£ nhÆ°ng thÆ°á»ng phá»¥ thuá»™c preprocessing (one-hot/encoding)</td>
  </tr>
</table>

**TÃ³m láº¡i chá»n gÃ¬?**
- Chá»n **LightGBM** náº¿u báº¡n Æ°u tiÃªn **tá»‘c Ä‘á»™**, **scale**, data lá»›n/nhiá»u feature.
- Chá»n **XGBoost** náº¿u báº¡n cáº§n **tÃ­nh á»•n Ä‘á»‹nh** cao, dataset nhá»-vá»«a, hoáº·c workflow tune Ä‘Ã£ quen.

> Gá»£i Ã½ thÃªm: náº¿u bÃ i toÃ¡n cÃ³ categorical â€œkhÃ³â€ (nhiá»u giÃ¡ trá»‹ hiáº¿m, high-cardinality), Ä‘Ã´i khi **CatBoost** lÃ  lá»±a chá»n ráº¥t Ä‘Ã¡ng thá»­.

---

## 12. Nhá»¯ng â€œbáº«yâ€ hay gáº·p & checklist debug

### 12.1 Báº«y phá»• biáº¿n
- ğŸ”¥ **Leakage** (Ä‘áº·c biá»‡t time-series): feature chá»©a thÃ´ng tin tÆ°Æ¡ng lai.
- ğŸŒ¿ `num_leaves` quÃ¡ lá»›n, thiáº¿u rÃ ng buá»™c â†’ overfit.
- ğŸ§ª Valid split sai:  
  - time-series mÃ  shuffle  
  - dá»¯ liá»‡u theo user mÃ  split láº«n user giá»¯a train/valid
- ğŸ·ï¸ Categorical sai: quÃªn `categorical_feature`.
- ğŸ¯ Metric khÃ´ng Ä‘Ãºng má»¥c tiÃªu (AUC vs F1 vs loglossâ€¦).

### 12.2 Checklist nhanh khi â€œÄ‘iá»ƒm tá»¥tâ€
- [ ] Split Ä‘Ãºng kiá»ƒu dá»¯ liá»‡u? (time/user/group)
- [ ] Early stopping cÃ³ dÃ¹ng chÆ°a?
- [ ] `num_leaves` cÃ³ quÃ¡ lá»›n khÃ´ng?
- [ ] `min_data_in_leaf` cÃ³ quÃ¡ nhá» khÃ´ng?
- [ ] Sampling (`feature_fraction`, `bagging_fraction`) Ä‘Ã£ báº­t?
- [ ] Categorical khai bÃ¡o Ä‘Ãºng?
- [ ] CÃ³ leakage trong feature engineering?

---

## 13. TÃ i liá»‡u tham kháº£o

- LightGBM Paper (NIPS 2017): â€œLightGBM: A Highly Efficient Gradient Boosting Decision Treeâ€
- Official LightGBM docs: Parameters / Tuning / Advanced Topics
- XGBoost docs & paper

<details>

</details>

---

<div align="center">
</div>