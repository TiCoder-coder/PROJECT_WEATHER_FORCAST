# Thuáº­t ToÃ¡n XGBoost Trong Dá»± BÃ¡o Thá»i Tiáº¿t

## ğŸ¯ Tá»•ng Quan Thuáº­t ToÃ¡n

XGBoost (Extreme Gradient Boosting) lÃ  thuáº­t toÃ¡n machine learning máº¡nh máº½ sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p **Gradient Boosting** Ä‘á»ƒ xÃ¢y dá»±ng mÃ´ hÃ¬nh dá»± bÃ¡o. Thuáº­t toÃ¡n nÃ y Ä‘áº·c biá»‡t hiá»‡u quáº£ cho cÃ¡c bÃ i toÃ¡n regression nhÆ° dá»± bÃ¡o thá»i tiáº¿t.

## ğŸ”„ CÃ¡ch Thuáº­t ToÃ¡n Hoáº¡t Äá»™ng

### 1. **Khá»Ÿi Táº¡o Model CÆ¡ Sá»Ÿ**
```
Fâ‚€(x) = argmin_Î³ âˆ‘áµ¢ L(yáµ¢, Î³)
```
- Báº¯t Ä‘áº§u vá»›i má»™t hÃ m dá»± Ä‘oÃ¡n Ä‘Æ¡n giáº£n (thÆ°á»ng lÃ  giÃ¡ trá»‹ trung bÃ¬nh)
- ÄÃ¢y lÃ  "base learner" Ä‘áº§u tiÃªn

### 2. **TÃ­nh Gradient (Äáº¡o HÃ m)**
```
gáµ¢ = âˆ‚L(yáµ¢, F(xáµ¢))/âˆ‚F(xáµ¢)
háµ¢ = âˆ‚Â²L(yáµ¢, F(xáµ¢))/âˆ‚FÂ²(xáµ¢)
```
- TÃ­nh first-order gradient (gáµ¢) vÃ  second-order gradient (háµ¢)
- Gradient cho biáº¿t hÆ°á»›ng cáº§n Ä‘iá»u chá»‰nh Ä‘á»ƒ giáº£m lá»—i

### 3. **XÃ¢y Dá»±ng Decision Tree**
```
Gain = Â½[âˆ‘(g_LÂ²/(h_L+Î»)) + âˆ‘(g_RÂ²/(h_R+Î»)) - âˆ‘(gÂ²/(h+Î»))] - Î³
```
- Chia dá»¯ liá»‡u thÃ nh cÃ¡c node dá»±a trÃªn gain function
- Sá»­ dá»¥ng second-order derivatives Ä‘á»ƒ tá»‘i Æ°u
- Regularization terms Î» vÃ  Î³ trÃ¡nh overfitting

### 4. **Cáº­p Nháº­t Model**
```
Fâ‚˜(x) = Fâ‚˜â‚‹â‚(x) + Î· * fâ‚˜(x)
```
- ThÃªm tree má»›i vÃ o model vá»›i learning rate Î·
- Î· thÆ°á»ng = 0.1 Ä‘á»ƒ trÃ¡nh overfitting

### 5. **Láº·p Láº¡i Cho Äáº¿n Convergence**
- Láº·p láº¡i bÆ°á»›c 2-4 cho Ä‘áº¿n khi Ä‘áº¡t sá»‘ trees tá»‘i Ä‘a
- Hoáº·c dá»«ng sá»›m náº¿u validation error khÃ´ng cáº£i thiá»‡n

## ğŸš€ Quy TrÃ¬nh Thá»±c Thi Trong Code

### **BÆ°á»›c 1: Chuáº©n Bá»‹ Dá»¯ Liá»‡u**
```python
def prepare_data(self, data_path, target_column, test_size=0.2):
    # 1. Äá»c dá»¯ liá»‡u thá»i tiáº¿t tá»« CSV
    df = pd.read_csv(data_path)

    # 2. Xá»­ lÃ½ missing values
    df = df.dropna()

    # 3. One-hot encoding cho categorical features
    X = pd.get_dummies(X, drop_first=True)

    # 4. Chia train/test
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)
```

### **BÆ°á»›c 2: Khá»Ÿi Táº¡o Model**
```python
def __init__(self, config=None):
    # Tham sá»‘ máº·c Ä‘á»‹nh cho thá»i tiáº¿t
    self.params = {
        'objective': 'reg:squarederror',  # MSE loss
        'eval_metric': 'rmse',           # Root Mean Square Error
        'max_depth': 6,                  # Äá»™ sÃ¢u cÃ¢y
        'learning_rate': 0.1,           # Tá»‘c Ä‘á»™ há»c
        'n_estimators': 100,            # Sá»‘ lÆ°á»£ng cÃ¢y
        'subsample': 0.8,              # Bootstrap sampling
        'colsample_bytree': 0.8,       # Feature sampling
    }
```

### **BÆ°á»›c 3: Training Process**
```python
def train(self, X_train, y_train):
    # 1. Scale features (Standardization)
    X_train_scaled = self.scaler.fit_transform(X_train)

    # 2. Táº¡o DMatrix (XGBoost's optimized data structure)
    dtrain = xgb.DMatrix(X_train_scaled, label=y_train)

    # 3. Train vá»›i early stopping
    self.model = xgb.train(
        self.params,
        dtrain,
        num_boost_round=100,
        early_stopping_rounds=10
    )
```

### **BÆ°á»›c 4: Prediction Process**
```python
def predict(self, X):
    # 1. Scale input features
    X_scaled = self.scaler.transform(X)

    # 2. Táº¡o DMatrix cho prediction
    dtest = xgb.DMatrix(X_scaled)

    # 3. Dá»± Ä‘oÃ¡n
    predictions = self.model.predict(dtest)

    return predictions
```

## ğŸ¯ HÆ°á»›ng Giáº£i Quyáº¿t BÃ i ToÃ¡n Dá»± BÃ¡o Thá»i Tiáº¿t

### **BÃ i ToÃ¡n**
- **Input**: Dá»¯ liá»‡u lá»‹ch sá»­ thá»i tiáº¿t (nhiá»‡t Ä‘á»™, Ä‘á»™ áº©m, Ã¡p suáº¥t, giÃ³...)
- **Output**: Dá»± bÃ¡o giÃ¡ trá»‹ thá»i tiáº¿t trong tÆ°Æ¡ng lai
- **Má»¥c tiÃªu**: Tá»‘i thiá»ƒu hÃ³a sai sá»‘ dá»± bÃ¡o

### **Chiáº¿n LÆ°á»£c Giáº£i Quyáº¿t**

#### **1. Feature Engineering**
```
Temperature(t) = f(Temperature(t-1), Humidity(t-1), Pressure(t-1), Wind(t-1), ...)
```
- Sá»­ dá»¥ng dá»¯ liá»‡u quÃ¡ khá»© Ä‘á»ƒ dá»± bÃ¡o tÆ°Æ¡ng lai
- Táº¡o lag features (t-1, t-2, t-3...)
- Seasonal decomposition

#### **2. Model Selection**
- XGBoost phÃ¹ há»£p vÃ¬:
  - Xá»­ lÃ½ Ä‘Æ°á»£c non-linear relationships
  - Robust vá»›i outliers
  - Feature importance built-in
  - Handle missing values

#### **3. Loss Function**
```
L(y, Å·) = (y - Å·)Â²  # MSE cho regression
```
- Penalize large errors heavily
- Differentiable for gradient descent

#### **4. Optimization**
```
Î¸* = argmin_Î¸ âˆ‘áµ¢ L(yáµ¢, F(xáµ¢; Î¸))
```
- Sá»­ dá»¥ng gradient descent Ä‘á»ƒ tá»‘i Æ°u
- Regularization Ä‘á»ƒ trÃ¡nh overfitting

## ğŸ“Š VÃ­ Dá»¥ Minh Há»a Quy TrÃ¬nh

### **Dá»¯ Liá»‡u Thá»i Tiáº¿t**
```
Date        | Temp | Humidity | Pressure | Wind | Temp_next_day
2024-01-01  | 25.5 | 65       | 1013    | 5.2  | 26.8
2024-01-02  | 26.8 | 70       | 1010    | 4.8  | 24.2
2024-01-03  | 24.2 | 75       | 1008    | 6.1  | 27.1
```

### **Quy TrÃ¬nh Training**
```
1. Fâ‚€(x) = 25.5 (mean temperature)
2. TÃ­nh residuals: ráµ¢ = yáµ¢ - Fâ‚€(xáµ¢)
3. XÃ¢y tree Ä‘áº§u tiÃªn fit residuals
4. Fâ‚(x) = Fâ‚€(x) + Î· * Treeâ‚(x)
5. Láº·p láº¡i vá»›i residuals má»›i
6. F_final(x) = Fâ‚€(x) + Î· * (Treeâ‚ + Treeâ‚‚ + ... + Treeâ‚™)
```

### **Prediction**
```
Input: [Temp=25.5, Humidity=65, Pressure=1013, Wind=5.2]
Output: Temp_next_day = 26.8Â°C
```

## ğŸ”§ Tham Sá»‘ Quan Trá»ng Trong Dá»± BÃ¡o Thá»i Tiáº¿t

| Tham sá»‘ | Ã nghÄ©a | GiÃ¡ trá»‹ Ä‘á» xuáº¥t |
|---------|---------|-----------------|
| `max_depth` | Äá»™ sÃ¢u cÃ¢y | 4-8 (trÃ¡nh overfitting) |
| `learning_rate` | Tá»‘c Ä‘á»™ há»c | 0.05-0.1 |
| `n_estimators` | Sá»‘ cÃ¢y | 100-500 |
| `subsample` | Tá»· lá»‡ máº«u | 0.8 (80% dá»¯ liá»‡u) |
| `colsample_bytree` | Tá»· lá»‡ features | 0.8 |

## ğŸ“ˆ ÄÃ¡nh GiÃ¡ Hiá»‡u Suáº¥t

### **Metrics ChÃ­nh**
- **RMSE**: Sai sá»‘ trung bÃ¬nh (Ä‘Æ¡n vá»‹ Â°C cho nhiá»‡t Ä‘á»™)
- **MAE**: Sai sá»‘ tuyá»‡t Ä‘á»‘i trung bÃ¬nh
- **RÂ²**: Há»‡ sá»‘ xÃ¡c Ä‘á»‹nh (0-1)

### **Trong Dá»± BÃ¡o Thá»i Tiáº¿t**
```
RMSE < 2Â°C: Tá»‘t cho nhiá»‡t Ä‘á»™
MAE < 1.5Â°C: Ráº¥t tá»‘t
RÂ² > 0.85: Model tá»‘t
```

## ğŸ¨ Visualization & Interpretability

### **Feature Importance**
```python
importance = model.get_feature_importance()
# Temperature(t-1): 35%
# Humidity(t-1): 25%
# Pressure(t-1): 20%
# Wind(t-1): 15%
# Other: 5%
```

### **Partial Dependence Plots**
- Hiá»ƒu áº£nh hÆ°á»Ÿng cá»§a tá»«ng feature
- Visualize non-linear relationships

## ğŸš€ Tá»‘i Æ¯u HÃ³a Cho Thá»i Tiáº¿t

### **1. Time Series Features**
- Lag features: Temp(t-1), Temp(t-2), Temp(t-3)
- Rolling statistics: Mean 7 days, Std 7 days
- Seasonal features: Month, Day of week

### **2. Domain Knowledge**
- Weather patterns: Monsoon, El NiÃ±o
- Geographical factors: Latitude, Longitude
- Historical trends: Climate change

### **3. Hyperparameter Tuning**
```python
# Grid Search cho thá»i tiáº¿t
param_grid = {
    'max_depth': [4, 6, 8],
    'learning_rate': [0.01, 0.05, 0.1],
    'n_estimators': [100, 200, 300]
}
```

## ğŸ”„ So SÃ¡nh Vá»›i CÃ¡c Thuáº­t ToÃ¡n KhÃ¡c

| Thuáº­t ToÃ¡n | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm | PhÃ¹ há»£p thá»i tiáº¿t |
|------------|---------|------------|-------------------|
| **XGBoost** | ChÃ­nh xÃ¡c cao, Robust | Cháº­m training | âœ… Ráº¥t tá»‘t |
| **Random Forest** | Nhanh, Ãt overfit | KhÃ´ng tá»‘i Æ°u | âš ï¸ Trung bÃ¬nh |
| **Linear Regression** | ÄÆ¡n giáº£n, Nhanh | Non-linear | âŒ KÃ©m |
| **LSTM** | Sequential data | Cáº§n nhiá»u data | âœ… Tá»‘t |

## ğŸ¯ Káº¿t Luáº­n

XGBoost giáº£i quyáº¿t bÃ i toÃ¡n dá»± bÃ¡o thá»i tiáº¿t báº±ng cÃ¡ch:

1. **Ensemble Learning**: Káº¿t há»£p nhiá»u weak learners
2. **Gradient Boosting**: Tá»‘i Æ°u tá»«ng bÆ°á»›c vá»›i gradient
3. **Regularization**: TrÃ¡nh overfitting
4. **Scalability**: Xá»­ lÃ½ big data hiá»‡u quáº£

**Káº¿t quáº£**: Model cÃ³ thá»ƒ dá»± bÃ¡o nhiá»‡t Ä‘á»™ vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao, giÃºp cáº£i thiá»‡n cháº¥t lÆ°á»£ng dá»± bÃ¡o thá»i tiáº¿t cho ngÆ°á»i dÃ¹ng.

---

*Thuáº­t toÃ¡n XGBoost Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh hiá»‡u quáº£ trong nhiá»u á»©ng dá»¥ng thá»±c táº¿, Ä‘áº·c biá»‡t lÃ  dá»± bÃ¡o thá»i tiáº¿t vá»›i Ä‘á»™ chÃ­nh xÃ¡c vÆ°á»£t trá»™i.*