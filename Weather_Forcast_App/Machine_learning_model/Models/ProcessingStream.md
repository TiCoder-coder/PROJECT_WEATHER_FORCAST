# üå¶Ô∏è Weather Forecast ‚Äì ML Models (Flow theo Source Code)

T√†i li·ªáu n√†y m√¥ t·∫£ **lu·ªìng x·ª≠ l√Ω (pipeline)** c·ªßa 4 thu·∫≠t to√°n ML trong project d·ª± b√°o th·ªùi ti·∫øt, d·ª±a tr√™n c√°c wrapper trong:

`Weather_Forcast_App/Machine_learning_model/Models/`

‚úÖ Models:
- üå≤ Random Forest (scikit-learn)
- üê± CatBoost
- üöÄ XGBoost
- üí° LightGBM

‚úÖ Test script:
- `Weather_Forcast_App/Machine_learning_model/TEST/test_ml_models.py`

---

## 0) T∆∞ duy thi·∫øt k·∫ø chung (ƒë·ªÉ UI/API g·ªçi th·ªëng nh·∫•t)

C√°c model wrapper ƒë·ªÅu c·ªë g·∫Øng chu·∫©n h√≥a theo c√πng m·ªôt ‚ÄúAPI c·∫£m gi√°c gi·ªëng nhau‚Äù:

### A. Task type
- `classification`: d·ª± ƒëo√°n nh√£n (Sunny/Cloudy/Rainy‚Ä¶)
- `regression`: d·ª± ƒëo√°n s·ªë (rain_mm, temperature‚Ä¶)

### B. Tr·∫°ng th√°i model
- `UNTRAINED` ‚Üí `TRAINED` (ho·∫∑c `FAILED` n·∫øu train l·ªói)

### C. Output chu·∫©n ho√°
**TrainingResult** th∆∞·ªùng c√≥:
- `success`: True/False
- `metrics`: dict (accuracy/f1‚Ä¶ ho·∫∑c rmse/mae/r2‚Ä¶)
- `training_time`
- `n_samples`, `n_features`
- `feature_names`
- `feature_importances` (n·∫øu model c√≥)
- `best_iteration` (n·∫øu c√≥ early stopping)
- `message`

**PredictionResult** th∆∞·ªùng c√≥:
- `predictions`
- `probabilities` (ch·ªâ classification + `return_proba=True`)
- `prediction_time`

üëâ M·ª•c ti√™u: t·∫ßng API/UI kh√¥ng c·∫ßn bi·∫øt chi ti·∫øt t·ª´ng library.

---

## 1) Pipeline chung (h·∫ßu h·∫øt model ƒë·ªÅu theo flow n√†y)

1) **Init**
- tr·ªôn params: default + user params  
- set `task_type`, `random_state`, flags (GPU‚Ä¶)
- chu·∫©n b·ªã metadata: schema feature, encoder, mapping category/datetime‚Ä¶

2) **Prepare Features (X)**
- ƒë·∫£m b·∫£o X l√† DataFrame
- x·ª≠ l√Ω NaN/inf
- x·ª≠ l√Ω datetime (n·∫øu c√≥)
- x·ª≠ l√Ω categorical (one-hot ho·∫∑c native categorical)
- ‚Äúƒë√≥ng bƒÉng schema‚Äù l√∫c train ƒë·ªÉ predict lu√¥n ƒë√∫ng c·ªôt

3) **Prepare Target (y)**
- regression: √©p float
- classification: LabelEncoder (string ‚Üí int), l∆∞u `classes_` ƒë·ªÉ decode ng∆∞·ª£c

4) **Split train/val**
- time-series: ∆∞u ti√™n `shuffle=False` ƒë·ªÉ tr√°nh leak theo th·ªùi gian
- classification tabular: c√≥ th·ªÉ `shuffle=True` + `stratify=True`

5) **Train**
- fit + early stopping/log (n·∫øu h·ªó tr·ª£)

6) **Evaluate**
- regression: RMSE/MAE/R2‚Ä¶
- classification: Accuracy/F1/Precision/Recall‚Ä¶

7) **Predict**
- preprocess X (fit=False) + align schema
- classification decode ng∆∞·ª£c v·ªÅ label g·ªëc

8) **Save/Load / Export artifacts**
- l∆∞u c·∫£ **model + preprocess metadata** ƒë·ªÉ inference ·ªïn ƒë·ªãnh

---

# üå≤ 2) Random Forest (`Random_Forest_Model.py`)

## 2.1 ƒêi·ªÉm ch√≠nh
- D√πng sklearn `RandomForestRegressor` / `RandomForestClassifier`
- H·ªó tr·ª£:
  - train + evaluate
  - predict (+ predict_proba cho classification)
  - feature importance
  - cross validate
  - save/load (joblib)

## 2.2 Flow chi ti·∫øt

### A) Train
1. Nh·∫≠n `X, y`
2. Chu·∫©n ho√° X (DataFrame/ndarray ‚Üí numeric)
3. N·∫øu classification: ƒë·∫£m b·∫£o y ƒë√∫ng d·∫°ng (c√≥ th·ªÉ encode)
4. Split train/val theo `validation_split`
5. Fit model
6. Evaluate (accuracy/f1 ho·∫∑c rmse/mae/r2‚Ä¶)
7. Tr·∫£ `TrainingResult`

### B) Predict
1. Chu·∫©n ho√° X gi·ªëng l√∫c train
2. Regression: `predict`
3. Classification:
   - `predict`
   - n·∫øu `return_proba=True` th√¨ g·ªçi `predict_proba`
4. Tr·∫£ `PredictionResult`

### C) Cross-validation
- sklearn `cross_val_score` (accuracy ho·∫∑c r2)

### D) Save/Load
- save: joblib dump model + metadata
- load: kh√¥i ph·ª•c model + tr·∫°ng th√°i `is_trained`

---

# üê± 3) CatBoost (`CatBoost_Model.py`)

## 3.1 ƒêi·ªÉm ch√≠nh
- M·∫°nh khi c√≥ nhi·ªÅu **categorical features** d·∫°ng string
- Kh√¥ng c·∫ßn one-hot l·ªõn nh∆∞ XGBoost
- H·ªó tr·ª£:
  - cat_features theo t√™n c·ªôt ho·∫∑c index
  - Pool + eval_set + early stopping
  - predict_proba
  - cv + grid_search
  - save/load

## 3.2 Flow chi ti·∫øt

### A) Train
1. Chu·∫©n ho√° X (DataFrame)
2. Chu·∫©n ho√° y:
   - regression: float
   - classification: c√≥ th·ªÉ encode / ƒë·∫£m b·∫£o ƒë√∫ng d·∫°ng
3. Resolve `cat_features` ‚Üí index ph√π h·ª£p CatBoost
4. T·∫°o `Pool(X, y, cat_features=...)`
5. N·∫øu c√≥ validation: t·∫°o `Pool` val
6. `model.fit(train_pool, eval_set=val_pool, ...)`
7. Evaluate ‚Üí metrics
8. Tr·∫£ `TrainingResult`

### B) Predict
- T·∫°o Pool t·ª´ X m·ªõi
- Classification:
  - `predict` + `predict_proba` (n·∫øu c·∫ßn)
- Regression:
  - `predict`

### C) CV + Tuning
- CV: `catboost.cv(...)`
- Tuning: `grid_search(...)`

### D) Save/Load
- save model (th∆∞·ªùng `.cbm`) + metadata (json) ƒë·ªÉ load/predict ·ªïn ƒë·ªãnh

---

# üöÄ 4) XGBoost (`XGBoost_Model.py`)

## 4.1 ƒêi·ªÉm ‚Äúnh·∫°y version‚Äù
B·∫°n ƒë√£ g·∫∑p l·ªói ki·ªÉu:
- `XGBModel.fit() got an unexpected keyword argument 'eval_metric'`
- `... early_stopping_rounds ...`

üëâ V√¨ **API `fit()` thay ƒë·ªïi theo phi√™n b·∫£n XGBoost** (ƒë·∫∑c bi·ªát b·∫£n b·∫°n c√†i l√† 3.x).
=> Wrapper c·∫ßn:
- set `eval_metric` trong params / set_params (ho·∫∑c callback ƒë√∫ng chu·∫©n),
- tr√°nh nh√©t `eval_metric`, `early_stopping_rounds` b·ª´a v√†o `.fit()`.

## 4.2 Flow x·ª≠ l√Ω (ƒë√∫ng chu·∫©n cho wrapper)

### A) Prepare Features (ƒë·∫∑c tr∆∞ng c·ªßa XGBoost wrapper)
1. Convert X ‚Üí DataFrame
2. Datetime ‚Üí t√°ch feature numeric (year/month/day/dow/hour/minute‚Ä¶)
3. Categorical ‚Üí **one-hot** (`pd.get_dummies`)
4. L∆∞u `feature_names` l√∫c train
5. Khi predict: **align schema**
   - thi·∫øu c·ªôt ‚Üí th√™m (0 ho·∫∑c NaN)
   - d∆∞ c·ªôt ‚Üí drop
   - reorder ƒë√∫ng th·ª© t·ª± train

### B) Prepare Target
- Regression: float
- Classification:
  - LabelEncoder string ‚Üí int
  - l∆∞u classes ƒë·ªÉ decode d·ª± ƒëo√°n

### C) Train
1. Preprocess X (fit=True), preprocess y
2. Split train/val (val_size)
3. Init model:
   - regression: `XGBRegressor`
   - classification: `XGBClassifier` (binary/multiclass)
4. Set params t∆∞∆°ng th√≠ch version (eval_metric, early stopping)
5. Fit
6. Evaluate + TrainingResult

### D) Predict
- preprocess X (fit=False) + align schema
- classification:
  - predict_proba n·∫øu c·∫ßn
  - predict ‚Üí decode v·ªÅ label g·ªëc
- regression: predict float

### E) Save/Load
- joblib dump:
  - model
  - feature_names
  - label_encoder
  - mapping datetime/categorical config

---

# üí° 5) LightGBM (`LightGBM_Model.py`)

## 5.1 ƒêi·ªÉm m·∫°nh wrapper LightGBM c·ªßa b·∫°n
Wrapper n√†y l√†m r·∫•t k·ªπ 3 th·ª© ƒë·ªÉ predict ‚Äúkh√¥ng l·ªách c·ªôt‚Äù:

### A) Schema freeze (`feature_names`)
- Sau khi preprocess xong l√∫c train, wrapper ‚Äúƒë√≥ng bƒÉng‚Äù danh s√°ch c·ªôt
- L√∫c predict/evaluate:
  - add missing cols
  - drop extra cols
  - reorder ƒë√∫ng schema

### B) Datetime feature extraction
N·∫øu c√≥ c·ªôt datetime:
- convert v·ªÅ datetime
- t√°ch:
  - `*_year, *_month, *_day, *_dow, *_hour, *_minute`
- drop c·ªôt datetime g·ªëc
- l∆∞u mapping trong `_datetime_feature_map`

### C) Categorical ·ªïn ƒë·ªãnh b·∫±ng `category` + set_categories
- fit=True:
  - cast sang `category`
  - l∆∞u categories list v√†o `_cat_categories`
- fit=False:
  - cast category
  - `set_categories(train_categories)` ƒë·ªÉ unseen category ‚Üí NaN (LightGBM x·ª≠ l√Ω ƒë∆∞·ª£c)

## 5.2 Flow chi ti·∫øt

### A) Train
1. `_prepare_features(X, fit=True)`:
   - replace inf ‚Üí NaN
   - datetime ‚Üí derived cols
   - categorical ‚Üí category + l∆∞u categories
   - set `feature_names`
2. `_prepare_target(y, fit=True)`:
   - regression float
   - classification LabelEncoder
3. Split train/val
4. `_init_model(n_classes)`:
   - classification multiclass set `num_class`
5. Fit:
   - d√πng callbacks:
     - `lgb.early_stopping(...)`
     - `lgb.log_evaluation(...)`
   - fallback n·∫øu version LightGBM kh√¥ng h·ªó tr·ª£ callbacks
6. Evaluate + feature importance
7. TrainingResult

### B) Predict
1. `_prepare_features(X, fit=False)`:
   - align schema/categories/datetime
2. predict:
   - regression float
   - classification decode label
3. PredictionResult

### C) Export artifacts ‚Äúlatest‚Äù
T·∫°o folder:
`Weather_Forcast_App/Machine_learning_artifacts/latest/`
- `Model.pkl`
- `Feature_list.json`
- `Metrics.json`
- `Train_info.json`

=> ƒë·ªÉ backend/service inference d√πng ngay.

---

## 6) G·ª£i √Ω chu·∫©n cho d·ªØ li·ªáu th·ªùi ti·∫øt (time-series)
- N·∫øu data theo th·ªùi gian: **shuffle=False**
- Tr√°nh leak: kh√¥ng shuffle tr∆∞·ªõc khi split
- N·∫øu c·∫ßn CV time-series: d√πng `TimeSeriesSplit` (kh√¥ng d√πng CV random)

---

## 7) Run test
```bash
python Weather_Forcast_App/Machine_learning_model/TEST/test_ml_models.py
